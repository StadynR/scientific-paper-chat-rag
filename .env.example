# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1
MEMORY_MODEL=llama3.2

# Embedding Model Configuration (Ollama model)
EMBEDDING_MODEL=mxbai-embed-large

# Chunk Configuration
# Smaller chunks = more precise retrieval
# Recommended: 800 for academic papers, 600 for dense technical content
CHUNK_SIZE=800
CHUNK_OVERLAP=150

# Retrieval Configuration
# Number of document chunks to retrieve
# More chunks = better coverage but slower
TOP_K_DOCUMENTS=6

# Generation Configuration
# Lower temperature = more deterministic/precise
# Higher temperature = more creative/diverse
# Recommended: 0.2-0.3 for factual/technical, 0.5-0.7 for creative
TEMPERATURE=0.3

# Vector Store Configuration
VECTOR_STORE_PATH=./data/vectorstore
MEMORY_STORE_PATH=./data/memory

# Application Configuration
MAX_UPLOAD_SIZE_MB=50
